{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Events Pipeline and Analysis\n",
    "### Project 3: Kumar, Jot, Patrick\n",
    "### MIDS W205: Summer 2021\n",
    "\n",
    "This project includes the creation of a pipeline for game data and an analysis of this data. The data is sourced from a Flask application that is also part of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Setup\n",
    "\n",
    "The pipeline utilizes Kafka -> Spark -> HDFS from docker containers. Run the following commands from the terminal for setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the docker-compose.yml file\n",
    "\n",
    "`docker-compose up -d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Kafka Topic\n",
    "\n",
    "```docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "    --create \\\n",
    "    --topic events \\\n",
    "    --partitions 1 \\\n",
    "    --replication-factor 1 \\\n",
    "    --if-not-exists \\\n",
    "    --zookeeper zookeeper:32181```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spin up the Flask Application\n",
    "\n",
    "```docker-compose exec mids \\\n",
    "  env FLASK_APP=/w205/project-3-kumar-narayanan/game_api.py \\\n",
    "  flask run --host 0.0.0.0```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data using Apache Bench from a Custom Bash Script\n",
    "\n",
    "`bash generate_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark for Transformations from Kafka\n",
    "\n",
    "Run the commands below in Jupyter Notebook, or execute `spark_commands.py` using:\n",
    "\n",
    "`docker-compose exec spark spark-submit /w205/project-3-kumar-narayanan/spark_commands.py`\n",
    "\n",
    "being sure to alter the path to the file as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Events from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\",\"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_events.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "| key|               value| topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 75 73 65 7...|events|        0|     0|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|     1|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|     2|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|     3|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|     4|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|     5|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|     6|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|     7|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|     8|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|     9|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|    10|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|    11|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|    12|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|    13|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|    14|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|    15|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|    16|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|    17|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|    18|2021-07-31 02:45:...|            0|\n",
      "|null|[7B 22 75 73 65 7...|events|        0|    19|2021-07-31 02:45:...|            0|\n",
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_events.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load as String and Filter on Events\n",
    "Here we load in the data to be human readable (string format) and filter only for events in the Kafka topic that are \"buy\" events. Also, we generate a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@udf('boolean')\n",
    "def is_valid_event(event_as_json):\n",
    "    \"\"\"Returns true if the event type is a buy, sell, delete, or play_game, otherwise returns False.\"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if (event['event_type'] == 'buy' or event['event_type'] == 'sell' or \n",
    "       event['event_type'] == 'play_game' or  event['event_type'] == 'delete'):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@udf('boolean')\n",
    "def is_purchase(event_as_json):\n",
    "    \"\"\"Returns true if the event type is a buy, otherwise returns False.\"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'buy':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@udf('boolean')\n",
    "def is_sell(event_as_json):\n",
    "    \"\"\"Returns true if the event type is a sell, otherwise returns False.\"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'sell':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@udf('boolean')\n",
    "def is_delete(event_as_json):\n",
    "    \"\"\"Returns true if the event type is a delete, otherwise returns False.\"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'delete':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@udf('boolean')\n",
    "def is_play(event_as_json):\n",
    "    \"\"\"Returns true if the event type is a play_game, otherwise returns False.\"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'play_game':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_valid_event('raw'))\n",
    "    \n",
    "extracted_all_events = all_events.rdd.map(lambda x: Row(timestamp=x.timestamp, **json.loads(x.raw))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "purchase_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_purchase('raw'))\n",
    "    \n",
    "extracted_events = purchase_events.rdd.map(lambda x: Row(timestamp=x.timestamp, **json.loads(x.raw))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sell_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_sell('raw'))\n",
    "    \n",
    "extracted_sell = sell_events.rdd.map(lambda x: Row(timestamp=x.timestamp, **json.loads(x.raw))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delete_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_delete('raw'))\n",
    "    \n",
    "extracted_delete = delete_events.rdd.map(lambda x: Row(timestamp=x.timestamp, **json.loads(x.raw))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "play_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_play('raw'))\n",
    "    \n",
    "extracted_play = play_events.rdd.map(lambda x: Row(timestamp=x.timestamp, **json.loads(x.raw))).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Schema and Show Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_all_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---------------+----+----------+-----+--------------------+---------------+\n",
      "|Accept|                Host|     User-Agent|desc|event_type| item|           timestamp|        user_id|\n",
      "+------+--------------------+---------------+----+----------+-----+--------------------+---------------+\n",
      "|   */*|agitated_darwin.v...|ApacheBench/2.3|None|       buy|knife|2021-07-31 02:45:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.v...|ApacheBench/2.3|None|       buy|knife|2021-07-31 02:45:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.v...|ApacheBench/2.3|None|       buy|knife|2021-07-31 02:45:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.v...|ApacheBench/2.3|None|       buy|knife|2021-07-31 02:45:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.v...|ApacheBench/2.3|None|       buy|knife|2021-07-31 02:45:...|agitated_darwin|\n",
      "+------+--------------------+---------------+----+----------+-----+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_all_events.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---------------+----+----------+-----+--------------------+---------------+\n",
      "|Accept|                Host|     User-Agent|desc|event_type| item|           timestamp|        user_id|\n",
      "+------+--------------------+---------------+----+----------+-----+--------------------+---------------+\n",
      "|   */*|agitated_darwin.v...|ApacheBench/2.3|None|       buy|knife|2021-07-31 02:45:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.v...|ApacheBench/2.3|None|       buy|knife|2021-07-31 02:45:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.v...|ApacheBench/2.3|None|       buy|knife|2021-07-31 02:45:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.v...|ApacheBench/2.3|None|       buy|knife|2021-07-31 02:45:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.v...|ApacheBench/2.3|None|       buy|knife|2021-07-31 02:45:...|agitated_darwin|\n",
      "+------+--------------------+---------------+----+----------+-----+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_events.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_sell.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---------------+----+----------+-----+--------------------+---------------+\n",
      "|Accept|                Host|     User-Agent|desc|event_type| item|           timestamp|        user_id|\n",
      "+------+--------------------+---------------+----+----------+-----+--------------------+---------------+\n",
      "|   */*|agitated_darwin.c...|ApacheBench/2.3|None|      sell|sword|2021-07-31 02:47:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.c...|ApacheBench/2.3|None|      sell|sword|2021-07-31 02:47:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.c...|ApacheBench/2.3|None|      sell|sword|2021-07-31 02:47:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.c...|ApacheBench/2.3|None|      sell|sword|2021-07-31 02:47:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.c...|ApacheBench/2.3|None|      sell|sword|2021-07-31 02:47:...|agitated_darwin|\n",
      "+------+--------------------+---------------+----+----------+-----+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_sell.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_delete.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---------------+----+----------+--------+--------------------+---------------+\n",
      "|Accept|                Host|     User-Agent|desc|event_type|    item|           timestamp|        user_id|\n",
      "+------+--------------------+---------------+----+----------+--------+--------------------+---------------+\n",
      "|   */*|agitated_darwin.c...|ApacheBench/2.3|  NA|    delete|crossbow|2021-07-31 02:47:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.c...|ApacheBench/2.3|  NA|    delete|crossbow|2021-07-31 02:47:...|agitated_darwin|\n",
      "|   */*|agitated_darwin.c...|ApacheBench/2.3|  NA|    delete|crossbow|2021-07-31 02:47:...|agitated_darwin|\n",
      "+------+--------------------+---------------+----+----------+--------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_delete.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_play.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---------------+----+----------+-----+--------------------+-------+\n",
      "|Accept|                Host|     User-Agent|desc|event_type| item|           timestamp|user_id|\n",
      "+------+--------------------+---------------+----+----------+-----+--------------------+-------+\n",
      "|   */*|agitated_darwin.c...|ApacheBench/2.3|None| play_game|sword|2021-07-31 02:47:...|  user9|\n",
      "|   */*|agitated_darwin.c...|ApacheBench/2.3|None| play_game|sword|2021-07-31 02:47:...|  user9|\n",
      "|   */*|agitated_darwin.c...|ApacheBench/2.3|None| play_game|sword|2021-07-31 02:47:...|  user9|\n",
      "|   */*|agitated_darwin.c...|ApacheBench/2.3|None| play_game|sword|2021-07-31 02:47:...|  user9|\n",
      "|   */*|agitated_darwin.c...|ApacheBench/2.3|None| play_game|sword|2021-07-31 02:47:...|  user9|\n",
      "+------+--------------------+---------------+----+----------+-----+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_play.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_all_events.write.mode(\"overwrite\").parquet(\"tmp/all_events\")\n",
    "extracted_events.write.mode(\"overwrite\").parquet(\"tmp/purchases\")\n",
    "extracted_sell.write.mode(\"overwrite\").parquet(\"tmp/sell\")\n",
    "extracted_delete.write.mode(\"overwrite\").parquet(\"tmp/delete\")\n",
    "extracted_play.write.mode(\"overwrite\").parquet(\"tmp/play\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct Analysis in Presto\n",
    "\n",
    "In order to run commands in Presto, execute the following script:\n",
    "\n",
    "  **docker-compose exec spark spark-submit /w205/project-3-kumar-narayanan/spark_commands.py**\n",
    "\n",
    "followed by\n",
    "  **docker-compose exec presto presto --server presto:8080 --catalog hive --schema default**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1 \n",
    "\n",
    "##### How many players are there? \n",
    "\n",
    "Command: \n",
    "```\n",
    "select count(distinct(user_id)) as distinct_users from all_events;\n",
    "```\n",
    "\n",
    "```\n",
    " presto:default> select count(distinct(user_id)) as distinct_users from all_events;\n",
    " distinct_users \n",
    "----------------\n",
    "              5 \n",
    "(1 row)\n",
    "\n",
    "Query 20210802_040930_00006_rk9vj, FINISHED, 1 node\n",
    "Splits: 3 total, 1 done (33.33%)\n",
    "0:01 [2.44K rows, 14.1KB] [3.52K rows/s, 20.4KB/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Query 2\n",
    "\n",
    "##### How many weapons are there? How many of each were sold? \n",
    "\n",
    "Command: \n",
    "```\n",
    "select count(*) as count, item from all_events group by item order by count(*) desc;\n",
    "```\n",
    "\n",
    "```\n",
    " presto:default> select count(*) as count, item from all_events group by item order by count(*) desc;\n",
    " count |   item   \n",
    "-------+----------\n",
    "  1220 | sword    \n",
    "   707 | crossbow \n",
    "   512 | knife    \n",
    "(3 rows)\n",
    "\n",
    "Query 20210802_040807_00005_rk9vj, FINISHED, 1 node\n",
    "Splits: 3 total, 0 done (0.00%)\n",
    "0:00 [0 rows, 0B] [0 rows/s, 0B/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Query 3\n",
    "\n",
    "##### How many times did each user participate in each event? \n",
    "\n",
    "Command: \n",
    "```\n",
    "select user_id, event_type, count(*) as count from all_events group by user_id, event_type order by count(*) desc;\n",
    "```\n",
    "\n",
    "```\n",
    "presto:default> select user_id, event_type, count(*) as count from all_events group by user_id, event_type order by count(*) desc;\n",
    "     user_id     | event_type | count \n",
    "-----------------+------------+-------\n",
    " agitated_darwin | buy        |   900 \n",
    " user9           | play_game  |   384 \n",
    " user7           | buy        |   384 \n",
    " user4           | buy        |   256 \n",
    " user5           | buy        |   256 \n",
    " agitated_darwin | sell       |   128 \n",
    " agitated_darwin | play_game  |   128 \n",
    " agitated_darwin | delete     |     3 \n",
    "(8 rows)\n",
    "\n",
    "Query 20210802_040614_00004_rk9vj, FINISHED, 1 node\n",
    "Splits: 3 total, 1 done (33.33%)\n",
    "0:00 [2.44K rows, 14.1KB] [5.22K rows/s, 30.3KB/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "\n",
    "##### How many (and what kind) of weapons do players own?\n",
    "\n",
    "Command: \n",
    "```\n",
    "select user_id, item as weapon, count(*) as count from all_events group by user_id, item order by count(*) DESC;\n",
    "```\n",
    "\n",
    "```\n",
    "presto:default> select user_id, item as weapon, count(*) as count from all_events group by user_id, item order by count(*) DESC;\n",
    "     user_id     |  weapon  | count \n",
    "-----------------+----------+-------\n",
    " agitated_darwin | sword    |   452 \n",
    " agitated_darwin | crossbow |   451 \n",
    " user7           | sword    |   384 \n",
    " user9           | sword    |   384 \n",
    " agitated_darwin | knife    |   256 \n",
    " user4           | knife    |   256 \n",
    " user5           | crossbow |   256 \n",
    "(7 rows)\n",
    "\n",
    "Query 20210802_040511_00003_rk9vj, FINISHED, 1 node\n",
    "Splits: 3 total, 2 done (66.67%)\n",
    "0:01 [0 rows, 0B] [0 rows/s, 0B/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### Query 5\n",
    "\n",
    "##### How many games were played with each kind of weapon?\n",
    "\n",
    "Command: \n",
    "```\n",
    "select user_id, count(*) as count, item as weapon from all_events where event_type = 'play_game' group by user_id, item;\n",
    "```\n",
    "\n",
    "```\n",
    "presto:default> select user_id, count(*) as count, item as weapon from all_events where event_type = 'play_game' group by user_id, item;\n",
    "     user_id     | count |  weapon  \n",
    "-----------------+-------+----------\n",
    " user9           |   384 | sword    \n",
    " agitated_darwin |   128 | crossbow \n",
    "(2 rows)\n",
    "\n",
    "Query 20210802_040328_00002_rk9vj, FINISHED, 1 node\n",
    "Splits: 3 total, 1 done (33.33%)\n",
    "0:06 [2.44K rows, 14.1KB] [433 rows/s, 2.51KB/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark streaming reads data\n",
    "\n",
    "- To start Spark Streaming Read run \n",
    "\n",
    "  **docker-compose exec spark spark-submit /w205/project-3-kumar-narayanan/filter_events_stream.py**\n",
    "\n",
    "- While the program is waiting feed data:\n",
    "\n",
    "  ** docker-compose exec mids \\\n",
    "     ab -n 50 -H \"Host: agitated_darwin.comcast.net\" http://localhost:5000/delete_item?item=crossbow **\n",
    "     \n",
    "\n",
    "  ** docker-compose exec mids \\\n",
    "     ab -n 50 -H \"Host: user8.comcast.net\" http://localhost:5000/sell_item?uid=user8%22item=knife **\n",
    "\n",
    "\n",
    "- The output of this data feed is as below:\n",
    "\n",
    "```\n",
    "+---------+---------+------+----+----------+----+----------+----+-------+\n",
    "|raw_event|timestamp|Accept|Host|User-Agent|desc|event_type|item|user_id|\n",
    "+---------+---------+------+----+----------+----+----------+----+-------+\n",
    "+---------+---------+------+----+----------+----+----------+----+-------+\n",
    "\n",
    "21/07/30 23:51:48 INFO StreamExecution: Streaming query made progress: {\n",
    "  \"id\" : \"77b9977e-f196-42bc-9475-f84b0eddfe65\",\n",
    "  \"runId\" : \"b060215b-6201-4b48-9e64-730c075a833d\",\n",
    "  \"name\" : null,\n",
    "  \"timestamp\" : \"2021-07-30T23:51:47.491Z\",\n",
    "  \"numInputRows\" : 49,\n",
    "  \"inputRowsPerSecond\" : 53.55191256830601,\n",
    "  \"processedRowsPerSecond\" : 79.9347471451876,\n",
    "  \"durationMs\" : {\n",
    "    \"addBatch\" : 503,\n",
    "    \"getBatch\" : 15,\n",
    "    \"getOffset\" : 1,\n",
    "    \"queryPlanning\" : 27,\n",
    "    \"triggerExecution\" : 613,\n",
    "    \"walCommit\" : 33\n",
    "  },\n",
    "  \"stateOperators\" : [ ],\n",
    "  \"sources\" : [ {\n",
    "    \"description\" : \"KafkaSource[Subscribe[events]]\",\n",
    "    \"startOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3237\n",
    "      }\n",
    "    },\n",
    "    \"endOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3286\n",
    "      }\n",
    "    },\n",
    "    \"numInputRows\" : 49,\n",
    "    \"inputRowsPerSecond\" : 53.55191256830601,\n",
    "    \"processedRowsPerSecond\" : 79.9347471451876\n",
    "  } ],\n",
    "  \"sink\" : {\n",
    "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@6ba51f99\"\n",
    "  }\n",
    "}\n",
    "21/07/30 23:51:48 INFO StreamExecution: Streaming query made progress: {\n",
    "  \"id\" : \"77b9977e-f196-42bc-9475-f84b0eddfe65\",\n",
    "  \"runId\" : \"b060215b-6201-4b48-9e64-730c075a833d\",\n",
    "  \"name\" : null,\n",
    "  \"timestamp\" : \"2021-07-30T23:51:48.167Z\",\n",
    "  \"numInputRows\" : 0,\n",
    "  \"inputRowsPerSecond\" : 0.0,\n",
    "  \"processedRowsPerSecond\" : 0.0,\n",
    "  \"durationMs\" : {\n",
    "    \"getOffset\" : 2,\n",
    "    \"triggerExecution\" : 2\n",
    "  },\n",
    "  \"stateOperators\" : [ ],\n",
    "  \"sources\" : [ {\n",
    "    \"description\" : \"KafkaSource[Subscribe[events]]\",\n",
    "    \"startOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3286\n",
    "      }\n",
    "    },\n",
    "    \"endOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3286\n",
    "      }\n",
    "    },\n",
    "    \"numInputRows\" : 0,\n",
    "    \"inputRowsPerSecond\" : 0.0,\n",
    "    \"processedRowsPerSecond\" : 0.0\n",
    "  } ],\n",
    "  \"sink\" : {\n",
    "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@6ba51f99\"\n",
    "  }\n",
    "}\n",
    "21/07/30 23:51:58 INFO StreamExecution: Streaming query made progress: {\n",
    "  \"id\" : \"77b9977e-f196-42bc-9475-f84b0eddfe65\",\n",
    "  \"runId\" : \"b060215b-6201-4b48-9e64-730c075a833d\",\n",
    "  \"name\" : null,\n",
    "  \"timestamp\" : \"2021-07-30T23:51:58.170Z\",\n",
    "  \"numInputRows\" : 0,\n",
    "  \"inputRowsPerSecond\" : 0.0,\n",
    "  \"processedRowsPerSecond\" : 0.0,\n",
    "  \"durationMs\" : {\n",
    "    \"getOffset\" : 1,\n",
    "    \"triggerExecution\" : 1\n",
    "  },\n",
    "  \"stateOperators\" : [ ],\n",
    "  \"sources\" : [ {\n",
    "    \"description\" : \"KafkaSource[Subscribe[events]]\",\n",
    "    \"startOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3286\n",
    "      }\n",
    "    },\n",
    "    \"endOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3286\n",
    "      }\n",
    "    },\n",
    "    \"numInputRows\" : 0,\n",
    "    \"inputRowsPerSecond\" : 0.0,\n",
    "    \"processedRowsPerSecond\" : 0.0\n",
    "  } ],\n",
    "  \"sink\" : {\n",
    "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@6ba51f99\"\n",
    "  }\n",
    "}\n",
    "21/07/30 23:52:08 INFO StreamExecution: Streaming query made progress: {\n",
    "  \"id\" : \"77b9977e-f196-42bc-9475-f84b0eddfe65\",\n",
    "  \"runId\" : \"b060215b-6201-4b48-9e64-730c075a833d\",\n",
    "  \"name\" : null,\n",
    "  \"timestamp\" : \"2021-07-30T23:52:08.173Z\",\n",
    "  \"numInputRows\" : 0,\n",
    "  \"inputRowsPerSecond\" : 0.0,\n",
    "  \"processedRowsPerSecond\" : 0.0,\n",
    "  \"durationMs\" : {\n",
    "    \"getOffset\" : 1,\n",
    "    \"triggerExecution\" : 1\n",
    "  },\n",
    "  \"stateOperators\" : [ ],\n",
    "  \"sources\" : [ {\n",
    "    \"description\" : \"KafkaSource[Subscribe[events]]\",\n",
    "    \"startOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3286\n",
    "      }\n",
    "    },\n",
    "    \"endOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3286\n",
    "      }\n",
    "    },\n",
    "    \"numInputRows\" : 0,\n",
    "    \"inputRowsPerSecond\" : 0.0,\n",
    "    \"processedRowsPerSecond\" : 0.0\n",
    "  } ],\n",
    "  \"sink\" : {\n",
    "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@6ba51f99\"\n",
    "  }\n",
    "}\n",
    "21/07/30 23:52:18 INFO StreamExecution: Streaming query made progress: {\n",
    "  \"id\" : \"77b9977e-f196-42bc-9475-f84b0eddfe65\",\n",
    "  \"runId\" : \"b060215b-6201-4b48-9e64-730c075a833d\",\n",
    "  \"name\" : null,\n",
    "  \"timestamp\" : \"2021-07-30T23:52:18.179Z\",\n",
    "  \"numInputRows\" : 0,\n",
    "  \"inputRowsPerSecond\" : 0.0,\n",
    "  \"durationMs\" : {\n",
    "    \"getOffset\" : 0,\n",
    "    \"triggerExecution\" : 0\n",
    "  },\n",
    "  \"stateOperators\" : [ ],\n",
    "  \"sources\" : [ {\n",
    "    \"description\" : \"KafkaSource[Subscribe[events]]\",\n",
    "    \"startOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3286\n",
    "      }\n",
    "    },\n",
    "    \"endOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3286\n",
    "      }\n",
    "    },\n",
    "    \"numInputRows\" : 0,\n",
    "    \"inputRowsPerSecond\" : 0.0\n",
    "  } ],\n",
    "  \"sink\" : {\n",
    "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@6ba51f99\"\n",
    "  }\n",
    "}\n",
    "\n",
    "+---------+---------+------+----+----------+----+----------+----+-------+\n",
    "|raw_event|timestamp|Accept|Host|User-Agent|desc|event_type|item|user_id|\n",
    "+---------+---------+------+----+----------+----+----------+----+-------+\n",
    "+---------+---------+------+----+----------+----+----------+----+-------+\n",
    "\n",
    "21/07/30 23:53:11 INFO StreamExecution: Streaming query made progress: {\n",
    "  \"id\" : \"77b9977e-f196-42bc-9475-f84b0eddfe65\",\n",
    "  \"runId\" : \"b060215b-6201-4b48-9e64-730c075a833d\",\n",
    "  \"name\" : null,\n",
    "  \"timestamp\" : \"2021-07-30T23:53:10.667Z\",\n",
    "  \"numInputRows\" : 48,\n",
    "  \"inputRowsPerSecond\" : 67.13286713286713,\n",
    "  \"processedRowsPerSecond\" : 117.64705882352942,\n",
    "  \"durationMs\" : {\n",
    "    \"addBatch\" : 293,\n",
    "    \"getBatch\" : 22,\n",
    "    \"getOffset\" : 0,\n",
    "    \"queryPlanning\" : 37,\n",
    "    \"triggerExecution\" : 408,\n",
    "    \"walCommit\" : 44\n",
    "  },\n",
    "  \"stateOperators\" : [ ],\n",
    "  \"sources\" : [ {\n",
    "    \"description\" : \"KafkaSource[Subscribe[events]]\",\n",
    "    \"startOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3288\n",
    "      }\n",
    "    },\n",
    "    \"endOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3336\n",
    "      }\n",
    "    },\n",
    "    \"numInputRows\" : 48,\n",
    "    \"inputRowsPerSecond\" : 67.13286713286713,\n",
    "    \"processedRowsPerSecond\" : 117.64705882352942\n",
    "  } ],\n",
    "  \"sink\" : {\n",
    "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@6ba51f99\"\n",
    "  }\n",
    "}\n",
    "21/07/30 23:53:11 INFO StreamExecution: Streaming query made progress: {\n",
    "  \"id\" : \"77b9977e-f196-42bc-9475-f84b0eddfe65\",\n",
    "  \"runId\" : \"b060215b-6201-4b48-9e64-730c075a833d\",\n",
    "  \"name\" : null,\n",
    "  \"timestamp\" : \"2021-07-30T23:53:11.111Z\",\n",
    "  \"numInputRows\" : 0,\n",
    "  \"inputRowsPerSecond\" : 0.0,\n",
    "  \"processedRowsPerSecond\" : 0.0,\n",
    "  \"durationMs\" : {\n",
    "    \"getOffset\" : 1,\n",
    "    \"triggerExecution\" : 1\n",
    "  },\n",
    "  \"stateOperators\" : [ ],\n",
    "  \"sources\" : [ {\n",
    "    \"description\" : \"KafkaSource[Subscribe[events]]\",\n",
    "    \"startOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3336\n",
    "      }\n",
    "    },\n",
    "    \"endOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3336\n",
    "      }\n",
    "    },\n",
    "    \"numInputRows\" : 0,\n",
    "    \"inputRowsPerSecond\" : 0.0,\n",
    "    \"processedRowsPerSecond\" : 0.0\n",
    "  } ],\n",
    "  \"sink\" : {\n",
    "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@6ba51f99\"\n",
    "  }\n",
    "}\n",
    "21/07/30 23:53:21 INFO StreamExecution: Streaming query made progress: {\n",
    "  \"id\" : \"77b9977e-f196-42bc-9475-f84b0eddfe65\",\n",
    "  \"runId\" : \"b060215b-6201-4b48-9e64-730c075a833d\",\n",
    "  \"name\" : null,\n",
    "  \"timestamp\" : \"2021-07-30T23:53:21.116Z\",\n",
    "  \"numInputRows\" : 0,\n",
    "  \"inputRowsPerSecond\" : 0.0,\n",
    "  \"durationMs\" : {\n",
    "    \"getOffset\" : 0,\n",
    "    \"triggerExecution\" : 0\n",
    "  },\n",
    "  \"stateOperators\" : [ ],\n",
    "  \"sources\" : [ {\n",
    "    \"description\" : \"KafkaSource[Subscribe[events]]\",\n",
    "    \"startOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3336\n",
    "      }\n",
    "    },\n",
    "    \"endOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 3336\n",
    "      }\n",
    "    },\n",
    "    \"numInputRows\" : 0,\n",
    "    \"inputRowsPerSecond\" : 0.0\n",
    "  } ],\n",
    "  \"sink\" : {\n",
    "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@6ba51f99\"\n",
    "  }\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State tracking - files grow as more events come in\n",
    "\n",
    "- To write data in streaming mode run\n",
    "  \n",
    "  **docker-compose exec spark spark-submit /w205/project-3-kumar-narayanan/write_events_stream.py**\n",
    "\n",
    "- With the program waiting feed data as below\n",
    "\n",
    "  while true; do\n",
    "    docker-compose exec mids \\\n",
    "      ab -n 10 -H \"Host: agitateddarwin.comcast.com\" \\\n",
    "        http://localhost:5000/play_game?weapon=crossbow\n",
    "\n",
    "  sleep 5\n",
    "\n",
    "    docker-compose exec mids \\\n",
    "      ab -n 10 -H \"Host: user8.comcast.net\" \\\n",
    "        http://localhost:5000/sell_item?uid=user8%22item=knife\n",
    "\n",
    "  sleep 5\n",
    "  \n",
    "  done\n",
    "\n",
    "\n",
    "- The output below captures the growing files as more data is fed.\n",
    "\n",
    "```\n",
    "(base) jupyter@w205-01:~/w205/project-3-kumar-jot-patrick$ docker-compose exec cloudera hadoop fs -ls /tmp/all_events\n",
    "Found 4 items\n",
    "drwxr-xr-x   - root supergroup          0 2021-07-30 23:46 /tmp/all_events/_spark_metadata\n",
    "-rw-r--r--   1 root supergroup       3381 2021-07-30 23:46 /tmp/all_events/part-00000-441e730c-9e79-4dc3-ab58-187487200b63-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3381 2021-07-30 23:46 /tmp/all_events/part-00000-48ab1ff9-7b73-4c0c-848b-4618e5f912b9-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup        919 2021-07-30 23:45 /tmp/all_events/part-00000-782a6ce0-5dfd-48b4-8920-de191c629d94-c000.snappy.parquet\n",
    "(base) jupyter@w205-01:~/w205/project-3-kumar-jot-patrick$ docker-compose exec cloudera hadoop fs -ls /tmp/all_events\n",
    "Found 5 items\n",
    "drwxr-xr-x   - root supergroup          0 2021-07-30 23:46 /tmp/all_events/_spark_metadata\n",
    "-rw-r--r--   1 root supergroup       3381 2021-07-30 23:46 /tmp/all_events/part-00000-441e730c-9e79-4dc3-ab58-187487200b63-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3381 2021-07-30 23:46 /tmp/all_events/part-00000-48ab1ff9-7b73-4c0c-848b-4618e5f912b9-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup        919 2021-07-30 23:45 /tmp/all_events/part-00000-782a6ce0-5dfd-48b4-8920-de191c629d94-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3379 2021-07-30 23:46 /tmp/all_events/part-00000-c974a689-d787-4602-8a78-9425556f1fbe-c000.snappy.parquet\n",
    "(base) jupyter@w205-01:~/w205/project-3-kumar-jot-patrick$ docker-compose exec cloudera hadoop fs -ls /tmp/all_events\n",
    "Found 6 items\n",
    "drwxr-xr-x   - root supergroup          0 2021-07-30 23:46 /tmp/all_events/_spark_metadata\n",
    "-rw-r--r--   1 root supergroup       3381 2021-07-30 23:46 /tmp/all_events/part-00000-441e730c-9e79-4dc3-ab58-187487200b63-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3381 2021-07-30 23:46 /tmp/all_events/part-00000-48ab1ff9-7b73-4c0c-848b-4618e5f912b9-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3378 2021-07-30 23:46 /tmp/all_events/part-00000-7452b8f0-85fb-42a0-a5fe-5371b5b09514-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup        919 2021-07-30 23:45 /tmp/all_events/part-00000-782a6ce0-5dfd-48b4-8920-de191c629d94-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3379 2021-07-30 23:46 /tmp/all_events/part-00000-c974a689-d787-4602-8a78-9425556f1fbe-c000.snappy.parquet\n",
    "(base) jupyter@w205-01:~/w205/project-3-kumar-jot-patrick$ docker-compose exec cloudera hadoop fs -ls /tmp/all_events\n",
    "Found 7 items\n",
    "drwxr-xr-x   - root supergroup          0 2021-07-30 23:46 /tmp/all_events/_spark_metadata\n",
    "-rw-r--r--   1 root supergroup       3381 2021-07-30 23:46 /tmp/all_events/part-00000-441e730c-9e79-4dc3-ab58-187487200b63-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3381 2021-07-30 23:46 /tmp/all_events/part-00000-48ab1ff9-7b73-4c0c-848b-4618e5f912b9-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup        919 2021-07-30 23:46 /tmp/all_events/part-00000-49d98d0b-eff6-4ca6-a744-62f63e5fdff7-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3378 2021-07-30 23:46 /tmp/all_events/part-00000-7452b8f0-85fb-42a0-a5fe-5371b5b09514-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup        919 2021-07-30 23:45 /tmp/all_events/part-00000-782a6ce0-5dfd-48b4-8920-de191c629d94-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3379 2021-07-30 23:46 /tmp/all_events/part-00000-c974a689-d787-4602-8a78-9425556f1fbe-c000.snappy.parquet\n",
    "(base) jupyter@w205-01:~/w205/project-3-kumar-jot-patrick$ docker-compose exec cloudera hadoop fs -ls /tmp/all_events\n",
    "Found 8 items\n",
    "drwxr-xr-x   - root supergroup          0 2021-07-30 23:47 /tmp/all_events/_spark_metadata\n",
    "-rw-r--r--   1 root supergroup       3381 2021-07-30 23:46 /tmp/all_events/part-00000-441e730c-9e79-4dc3-ab58-187487200b63-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3381 2021-07-30 23:46 /tmp/all_events/part-00000-48ab1ff9-7b73-4c0c-848b-4618e5f912b9-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup        919 2021-07-30 23:46 /tmp/all_events/part-00000-49d98d0b-eff6-4ca6-a744-62f63e5fdff7-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3382 2021-07-30 23:47 /tmp/all_events/part-00000-5989a8bb-a2af-4f31-a725-44c2ce0870f3-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3378 2021-07-30 23:46 /tmp/all_events/part-00000-7452b8f0-85fb-42a0-a5fe-5371b5b09514-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup        919 2021-07-30 23:45 /tmp/all_events/part-00000-782a6ce0-5dfd-48b4-8920-de191c629d94-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3379 2021-07-30 23:46 /tmp/all_events/part-00000-c974a689-d787-4602-8a78-9425556f1fbe-c000.snappy.parquet\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
